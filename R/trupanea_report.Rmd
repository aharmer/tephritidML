---
title: "*Trupanea* identification using CNN"
author: "Aaron Harmer"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: yes
    fig_caption: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r message = FALSE, warning = FALSE}
if(!require(pacman)) install.packages("pacman")
pacman::p_load(tidyverse, pander, patchwork, terra, viridis, reticulate, kableExtra)

source_python("C:/Users/HarmerA/OneDrive - MWLR/repos/tephritidML/python/train_test_functions.py")

dat1 = read_csv("C:/Users/HarmerA/OneDrive - MWLR/repos/tephritidML/logs/trupanea_v1_1_Xception_200_epoch_transfer_earlyStop_log.csv") %>% 
  pivot_longer(!epoch, names_to = "stat", values_to = "value")
dat2 = read_csv("C:/Users/HarmerA/OneDrive - MWLR/repos/tephritidML/logs/trupanea_v1_2_Xception_200_epoch_transfer_earlyStop_log.csv") %>% 
  pivot_longer(!epoch, names_to = "stat", values_to = "value")
dat3 = read_csv("C:/Users/HarmerA/OneDrive - MWLR/repos/tephritidML/logs/trupanea_v1_3_Xception_200_epoch_transfer_earlyStop_log.csv") %>% 
  pivot_longer(!epoch, names_to = "stat", values_to = "value")
```

## Summary

A convolution neural network was trained via transfer learning to identify *Trupanea* species using wing images. Additional tephritids were included but only identified to genus. Threefold cross-validation was used to achieve an accuracy of 94% (472/500) at predicting class. The image set comprised 500 images in 13 classes. The image set was sorted into three folds using stratified round-robin cross-validation. Each set had a 2/3 training, 1/3 test split. While overall accuracy was high, the model did not perform well at predicting some individual *Trupanea* species (namely *T. centralis*, *T. completa*, *T. extensa* and *T. vittigera*), likely due to the low number of images in these classes.

A final model was trained using the full set of 500 images (i.e. no validation set). The full model was used to predict the species identity of 370 previously unidentified *Trupanea* images. In general, the model performed reasonably well at predicting the correct species, at least based on visual inspection. The majority of apparent mis-indentifications were for badly damaged wings (e.g. NZAC04254210), with the exception of two that were identified as *Bactrocera* (NZAC04254333 and NZAC04254334), which were clearly wrong. These two specimens and several other visually very similar images are possibly *T. longipennis*, or perhaps another species not previously known in the dataset. Interestingly, the model identified an image as *Austrotephritis* (but is possibly *Sphenella*) which appears to have been included in the unidentified *Trupanea* set by mistake.

<br>

### Image pre-processing

Images were prepared by:

-   Converting colour images to grayscale
-   Blurring to reduce image grain noise
-   Standardising brightness and contrast to mean = 0.5, contrast range = ±2 standard deviations, with the extreme values clipped
-   Squashing and downsampling to 299 by 299 pixels to match the network input filter size

<br>

### Model training

Transfer learning was used to retrain an Xception model initially trained on the Imagenet dataset. The model was retrained three times, each run was initially set for 200 epochs but with an early-stopping callback and learning rate of 1e-04. Each model was fine-tuned at a learning rate of 1e-05, again with early-stopping set.

Images were augmented to prevent over-fitting and increase the robustness of the model to variation in additional images added later. Image augmentations included:

-   Random horizontal and vertical shift up to 10%
-   Random zoom up to ±10%
-   Random rotation up to ±25 degrees

<br>

## Results

The cross-validation models achieved an average accuracy of 94% (472 correct/28 wrong) across the three runs. Each model stopped before reaching 200 epochs with accuracy and loss both plateauing (Figure 1).

<br>

```{r warning = FALSE, fig.width = 12, fig.cap = "Fig. 1: Accuracy and loss of the three cross-validated Xception models.", fig.align="center"}
acc_plot = dat1 %>% 
  filter(stat == "accuracy" | stat == "val_accuracy") %>% 
  ggplot(aes(x = epoch, y = value, colour = stat)) +
    geom_line(linewidth = 1) +
    geom_line(data = subset(dat2, stat == "accuracy" | stat == "val_accuracy"), aes(x = epoch, y = value, colour = stat), linewidth = 1) +
    geom_line(data = subset(dat3, stat == "accuracy" | stat == "val_accuracy"), aes(x = epoch, y = value, colour = stat), linewidth = 1) +
    scale_colour_manual(labels = c("Training accuracy", "Validation accuracy"), values = c("#70ad47", "#4472c4")) +
    ylim(c(0.5,1)) +
    xlab("\nEpoch") +
    ylab("Accuracy\n") +
    theme_classic(base_size = 16) +
    theme(legend.title = element_blank(), legend.position = c(0.75, 0.25))

loss_plot = dat1 %>% 
  filter(stat == "loss" | stat == "val_loss") %>% 
  ggplot(aes(x = epoch, y = value, colour = stat)) +
    geom_line(linewidth = 1) +
    geom_line(data = subset(dat2, stat == "loss" | stat == "val_loss"), aes(x = epoch, y = value, colour = stat), linewidth = 1) +
    geom_line(data = subset(dat3, stat == "loss" | stat == "val_loss"), aes(x = epoch, y = value, colour = stat), linewidth = 1) +
    scale_colour_manual(labels = c("Training loss", "Validation loss"), values = c("#70ad47", "#4472c4")) +
    xlab("\nEpoch") +
    ylab("Loss\n") +
    theme_classic(base_size = 16) +
    theme(legend.title = element_blank(), legend.position = c(0.75, 0.75))

acc_plot|loss_plot
```

<br>

### Classification report for each class

Validation accuracy varied between classes, which was to be expected given the relatively small dataset for some classes.

```{python include = FALSE}
from sklearn.metrics import classification_report
from pandas import DataFrame

DATASET_PATH = 'C:/Users/harmera/OneDrive - MWLR/repos/tephritidML/'
model_dir = DATASET_PATH + 'models/'
model_name = 'Xception'
labels_path = DATASET_PATH + 'labels/labels3.csv'

results = []
for i in range(1,4):
  model_file = model_dir + 'trupanea_v1_{}_Xception_transfer.h5'.format(i)
  images_path = DATASET_PATH + 'img/trupanea_model/img_folds/{}/'.format(i)
  test_data_dir = images_path + 'val/'
  _, answers = test_model(model_file, labels_path, test_data_dir, model_name)
  results += answers
 
predicted = [tup[1] for tup in results]
y_test = [tup[0] for tup in results]
file_name = [tup[3] for tup in results]
score = [tup[2] for tup in results]

report = classification_report(y_test, predicted, output_dict = True)
report = DataFrame(report).transpose()
```

```{r message = FALSE}
report = py$report
report[nrow(report)-2,] = c(NA, NA, report[nrow(report)-2,3], report[nrow(report)-1,4])  # Because of blank line for global accuracy in report
report = report %>%
  rownames_to_column(var = "class") %>%
  mutate_if(is.numeric, round, digits = 2)

kableExtra::kable(report) %>% 
          kableExtra::kable_styling(full_width = FALSE)
```

<br>

### Incorrect identifications during validation
The 28 images in the table below were incorrectly identified during validation.

```{r message = FALSE}
DATASET_PATH = 'C:/Users/harmera/OneDrive - MWLR/repos/tephritidML/'
model_dir = paste0(DATASET_PATH, 'models/')
model_path = paste0(model_dir, 'trupanea_v1_1_Xception_transfer.h5')

path = 'C:/Users/HarmerA/OneDrive - MWLR/repos/tephritidML/img/trupanea_model/img_sorted'
subdirs = tibble(class = list.dirs(path, recursive = FALSE, full.names = FALSE), path = list.dirs(path, recursive = FALSE))
filenames = subdirs %>%
   mutate(first_file = map_chr(path, ~list.files(.x, full.names = TRUE)[1]))

misID = data.frame(file = py$file_name, actual = py$y_test, pred = py$predicted, score = unlist(py$score)) %>% 
  filter(pred != actual)

misFile = c()
for (i in 1:nrow(misID)) {
  file = paste0('C:/Users/HarmerA/OneDrive - MWLR/repos/tephritidML/img/trupanea_model/img_sorted/', misID[i,1])
  misFile = c(misFile, file)
}

prediction_table = data.frame()
for (i in 1:nrow(misID)) {
  test_file = misFile[i]
  prediction_exemplar_file = filenames[,3][filenames[,1] == misID[i,3]]
  test_img = paste0(pandoc.image.return(test_file), "{width=100px}")
  prediction_exemplar_img = paste0(pandoc.image.return(prediction_exemplar_file), "{width=100px}")
  row = cbind(file = substring(misID[i,1], regexpr("N", misID[i,1])) %>% substring(1, 12), round(misID[i,4], 2), misID[i,2], test_img, misID[i,3], prediction_exemplar_img)
  prediction_table = rbind(prediction_table, row)
}

names(prediction_table) = c("file", "score", "actual", "actual_image", "predicted", "prediction_exemplar")
kableExtra::kable(prediction_table) %>%
          kableExtra::kable_styling(full_width = FALSE)
```

<br>

### Predictions for unknown *Trupanea* species.
A final model was retrained using the entire 500 image set (i.e. no training/validation split). The model was then used to predict the species identity for 370 previously unidentified *Trupanea* images. The table below shows the model predictions for the novel image set, with the wing image being tested compared against an example of the predicted species identification.
```{python include = FALSE}
DATASET_PATH = 'C:/Users/harmera/OneDrive - MWLR/repos/tephritidML/'
model_dir = DATASET_PATH + 'models/'
model_path = model_dir + 'trupanea_v2_full_Xception_transfer.h5'
model_name = 'Xception'
labels_path = DATASET_PATH + 'labels/labels3.csv'
test_images_path = DATASET_PATH + 'img/trupanea_model/img_unk/'

predID = predict_new(model_path, labels_path, test_images_path, model_name)
```

``` {r message = FALSE}
predID = py$predID

tab = c()
for (i in 1:length(predID[[2]])) {
  vals = unlist(predID[[2]][[i]][c(5,2:4)])
  tab = rbind(tab, vals)
}

tab = as.data.frame(tab, row.names = FALSE)
names(tab) = c("file", "predicted_id", "prediction_score", "prediction_margin")

newID_table = data.frame()
for (i in 1:nrow(tab)) {
  test_file = tab[i,1]
  newID_exemplar_file = filenames[,3][filenames[,1] == tab[i,2]]
  test_img = paste0(pandoc.image.return(test_file), "{width=100px}")
  newID_exemplar_img = paste0(pandoc.image.return(newID_exemplar_file), "{width=100px}")
  row = cbind(file = strsplit(test_file, "\\\\")[[1]][3], test_img, round(as.numeric(tab[i,3]), 2), round(as.numeric(tab[i,4]), 1), tab[i,2], newID_exemplar_img)
  newID_table = rbind(newID_table, row)
}

names(newID_table) = c("file", "test_image", "prediction_score", "prediction_margin", "predicted_ID", "prediction_exemplar")
kableExtra::kable(newID_table) %>% 
          kableExtra::kable_styling(full_width = FALSE)
```

<br>

### Representative activation maps for each class

Value for each species is the training accuracy. I am uncertain how to interpret the activation maps. While the wing itself appears to be the most salient object in each image, attention to the finer details within each wing may be needed for better model performance. Is this a result of the small dataset?

<br>

```{r warning = FALSE, message = FALSE, figures-side, fig.show="hold", out.width="50%"}
source_python('C:/Users/HarmerA/OneDrive - MWLR/repos/tephritidML/python/exp/act_maps.py')

labels = as.vector(unlist(read_csv("C:/Users/HarmerA/OneDrive - MWLR/repos/tephritidML/labels/labels3.csv", col_names = FALSE)))

for (i in 1:nrow(filenames)) {
  heatmap = activation(model_path, as.character(filenames[i,3]))
  idx = which.max(heatmap[[1]])
  name = labels[idx]
  p = paste0(round(max(heatmap[[1]]) * 100, digits = 1), "%")
  
  flattened = (heatmap[[2]][,,1]*0.3) + (heatmap[[2]][,,2]*0.59) + (heatmap[[2]][,,3]*0.11)
  
  plot(rast(flattened), col = gray.colors(256), legend = FALSE, axes = FALSE)
  plot(rast(heatmap[[3]][,,2]), legend = FALSE, axes = FALSE, col = viridis(256), alpha = 0.4, add = TRUE)
  text(x = 1, y = 285, labels = name, pos = 4, cex = 1)
  text(x = 1, y = 270, labels = p, pos = 4, cex = 1)
}
```
